{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6R6IawQjR3CAA5ehnf5rJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harishkulkarni10/ecommerce-session-recommender/blob/main/3_Baseline_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Notebook 3 ‚Äî Baseline Models\n",
        "\n",
        "**Goal:**  \n",
        "To build and evaluate non-neural baseline recommenders as performance benchmarks for future deep models.\n",
        "\n",
        "### Why Baselines Matter\n",
        "Before building GRU or Transformer models, it‚Äôs important to know how simple models perform.  \n",
        "If a deep model can‚Äôt beat a simple popularity or co-occurrence baseline, it‚Äôs not worth deploying.\n",
        "\n",
        "### Baselines Implemented\n",
        "1. **Popularity-Based Recommender** ‚Äî Recommends globally popular products.  \n",
        "2. **Item-Based kNN Recommender** ‚Äî Recommends products similar to those viewed in the session (based on co-occurrence).\n",
        "\n",
        "### Evaluation Metrics\n",
        "- **Recall@K:** Whether the true next item appears in the top-K recommendations.  \n",
        "- **MRR@K:** How highly ranked the true next item is among recommendations.\n",
        "\n",
        "### Outputs\n",
        "- Baseline metrics (`baseline_metrics.json`)\n",
        "- Example predictions (`baseline_predictions.csv`)\n",
        "- Ready for deep model benchmarking in Notebook 4.\n"
      ],
      "metadata": {
        "id": "ou-yOX79Trz8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KZg_4OTBTo7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15750883-a568-441e-a67e-ddd370b1eaba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Loaded sessions: 16397 total\n",
            "Split info keys: ['train', 'val', 'test']\n",
            "Train sessions: 13117, Val: 1640, Test: 1640\n",
            "\n",
            "üõç Example Session 1 ‚Üí [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] (showing first 10 items)\n"
          ]
        }
      ],
      "source": [
        "# STEP 3.1 ‚Äî Mount Drive and load session data\n",
        "\n",
        "from google.colab import drive\n",
        "import os, pickle, pandas as pd\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Define project root\n",
        "PROJECT_ROOT = '/content/drive/MyDrive/Data Science course/Major Projects/Projects/e-commerce recommender/diginetica_recommender_project/'\n",
        "\n",
        "# Define paths\n",
        "SESSION_PATH = os.path.join(PROJECT_ROOT, 'data/sessions/')\n",
        "CLEANED_PATH = os.path.join(PROJECT_ROOT, 'data/cleaned/')\n",
        "\n",
        "# Load session data\n",
        "with open(os.path.join(SESSION_PATH, 'sessions.pkl'), 'rb') as f:\n",
        "    sessions = pickle.load(f)\n",
        "\n",
        "# Load split info (train/val/test)\n",
        "with open(os.path.join(SESSION_PATH, 'split.pkl'), 'rb') as f:\n",
        "    split = pickle.load(f)\n",
        "\n",
        "print(f\"Loaded sessions: {len(sessions)} total\")\n",
        "print(\"Split info keys:\", list(split.keys()))\n",
        "print(f\"Train sessions: {len(split['train'])}, Val: {len(split['val'])}, Test: {len(split['test'])}\")\n",
        "\n",
        "# Quick check\n",
        "sample_sid = list(sessions.keys())[0]\n",
        "print(f\"\\nüõç Example Session {sample_sid} ‚Üí {sessions[sample_sid][:10]} (showing first 10 items)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 3.2 Popularity Based Recommender\n",
        "from collections import Counter\n",
        "\n",
        "# Extract train sessions\n",
        "train_sids = set(split['train'])\n",
        "\n",
        "# Count item frequencies across all \"train\" sessions\n",
        "pop_counter = Counter()\n",
        "for sid in train_sids:\n",
        "  items = sessions.get(sid)\n",
        "  if items:\n",
        "    pop_counter.update(items)\n",
        "\n",
        "# Sort items by descending order of popularity\n",
        "popular_items = [item for item, _ in pop_counter.most_common()]\n",
        "\n",
        "print(f\"Counted {len(popular_items)} unique items from training sessions...\")\n",
        "print(f\"Top 10 most popular item indices: {popular_items[:10]}\")\n",
        "\n",
        "# Quick look at item frequencies\n",
        "top10 = list(pop_counter.most_common(10))\n",
        "print(\"\\nTop 10 Items (item_idx, count):\")\n",
        "for itm, cnt in top10:\n",
        "    print(f\"Item {itm:6d} ‚Üí viewed {cnt} times\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKw_ttfULczy",
        "outputId": "4028acdd-d645-421e-c39b-f86ad2c4a60e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counted 25970 unique items from training sessions...\n",
            "Top 10 most popular item indices: [18, 557, 1003, 894, 1392, 86, 896, 1309, 1049, 2985]\n",
            "\n",
            "Top 10 Items (item_idx, count):\n",
            "Item     18 ‚Üí viewed 89 times\n",
            "Item    557 ‚Üí viewed 73 times\n",
            "Item   1003 ‚Üí viewed 69 times\n",
            "Item    894 ‚Üí viewed 64 times\n",
            "Item   1392 ‚Üí viewed 62 times\n",
            "Item     86 ‚Üí viewed 60 times\n",
            "Item    896 ‚Üí viewed 55 times\n",
            "Item   1309 ‚Üí viewed 55 times\n",
            "Item   1049 ‚Üí viewed 54 times\n",
            "Item   2985 ‚Üí viewed 54 times\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 3.3 Evaluate\n",
        "import numpy as np, json, pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# Evaluation functions\n",
        "def recall_at_k(preds, target, k):\n",
        "    return 1.0 if target in preds[:k] else 0.0\n",
        "\n",
        "def mrr_at_k(preds, target, k):\n",
        "    for i, p in enumerate(preds[:k], start=1):\n",
        "        if p == target:\n",
        "            return 1.0 / i\n",
        "    return 0.0\n",
        "\n",
        "# Build test examples: (session, input_sequence, target_item)\n",
        "test_sids = set(split['test'])\n",
        "test_examples = []\n",
        "for sid in test_sids:\n",
        "    seq = sessions.get(sid)\n",
        "    if not seq or len(seq) < 2:\n",
        "        continue\n",
        "    input_seq = seq[:-1]  # context (history)\n",
        "    target = seq[-1]      # true next item\n",
        "    test_examples.append((sid, input_seq, target))\n",
        "\n",
        "print(f\"Prepared {len(test_examples)} test examples.\")\n",
        "\n",
        "# Evaluate on test set\n",
        "K = 20\n",
        "recalls, mrrs = [], []\n",
        "for sid, inp, tgt in tqdm(test_examples, desc=\"Evaluating Popularity Model\"):\n",
        "    preds = popular_items  # same for all sessions\n",
        "    recalls.append(recall_at_k(preds, tgt, K))\n",
        "    mrrs.append(mrr_at_k(preds, tgt, K))\n",
        "\n",
        "# Compute metrics\n",
        "recall_k = np.mean(recalls)\n",
        "mrr_k = np.mean(mrrs)\n",
        "\n",
        "# Save metrics\n",
        "results_dir = os.path.join(PROJECT_ROOT, 'results')\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "pop_metrics = {\n",
        "    \"model\": \"popularity_baseline\",\n",
        "    \"K\": K,\n",
        "    \"recall@K\": round(float(recall_k), 4),\n",
        "    \"mrr@K\": round(float(mrr_k), 4),\n",
        "    \"num_test_sessions\": len(test_examples)\n",
        "}\n",
        "\n",
        "metrics_fp = os.path.join(results_dir, 'baseline_metrics.json')\n",
        "with open(metrics_fp, 'w') as f:\n",
        "    json.dump(pop_metrics, f, indent=2)\n",
        "print(f\"Metrics saved to: {metrics_fp}\")\n",
        "\n",
        "# save sample predictions\n",
        "sample_preds = pd.DataFrame({\n",
        "    \"session_id\": [sid for sid, _, _ in test_examples[:10]],\n",
        "    \"ground_truth\": [tgt for _, _, tgt in test_examples[:10]],\n",
        "    \"top5_predictions\": [popular_items[:5]] * 10\n",
        "})\n",
        "sample_preds_fp = os.path.join(results_dir, 'baseline_predictions.csv')\n",
        "sample_preds.to_csv(sample_preds_fp, index=False)\n",
        "print(f\"Sample predictions saved to: {sample_preds_fp}\")\n",
        "\n",
        "# results\n",
        "print(\"\\nPopularity Baseline Results\")\n",
        "for k, v in pop_metrics.items():\n",
        "    print(f\"{k:20}: {v}\")"
      ],
      "metadata": {
        "id": "aTY4MHFwScpu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8736e3b9-4cca-44eb-e236-e819ada6e3df"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepared 1640 test examples.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating Popularity Model: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1640/1640 [00:00<00:00, 333931.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics saved to: /content/drive/MyDrive/Data Science course/Major Projects/Projects/e-commerce recommender/diginetica_recommender_project/results/baseline_metrics.json\n",
            "Sample predictions saved to: /content/drive/MyDrive/Data Science course/Major Projects/Projects/e-commerce recommender/diginetica_recommender_project/results/baseline_predictions.csv\n",
            "\n",
            "Popularity Baseline Results\n",
            "model               : popularity_baseline\n",
            "K                   : 20\n",
            "recall@K            : 0.0116\n",
            "mrr@K               : 0.0028\n",
            "num_test_sessions   : 1640\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary ‚Äî Popularity-Based Recommender\n",
        "\n",
        "**Objective:**  \n",
        "Build a simple global popularity model as a baseline for evaluating more advanced recommender systems.\n",
        "\n",
        "### Key Steps\n",
        "1. Counted item occurrences across all **training sessions**.  \n",
        "2. Ranked items by frequency to obtain a **Top-K popularity list**.  \n",
        "3. Evaluated model on test sessions using:\n",
        "   - **Recall@20** ‚Äî 0.0116  \n",
        "   - **MRR@20** ‚Äî 0.0028  \n",
        "\n",
        "### Insights\n",
        "- This baseline reflects non-personalized ‚ÄúTop Trending Items.‚Äù  \n",
        "- Despite simplicity, it provides a strong **benchmark floor**.  \n",
        "- All future models (GRU4Rec, SASRec) should outperform these numbers to justify added complexity.\n",
        "\n",
        "### Outputs\n",
        "| File | Description |\n",
        "|------|--------------|\n",
        "| `results/baseline_metrics.json` | Stored evaluation metrics for comparison |\n",
        "| `results/baseline_predictions.csv` | Example Top-K predictions for test sessions |\n",
        "\n",
        "Next ‚Üí **Item-Based kNN Recommender**, which introduces **session-level personalization** by leveraging item co-occurrence patterns.\n"
      ],
      "metadata": {
        "id": "h8-NcZGkfzVc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nkD61Z4beFGa"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Item based kNN Recommender**\n",
        "|               |                                                             |\n",
        "| -------------------- | ----------------------------------------------------------------------- |\n",
        "| **Goal**             | Recommend items similar to those in the current session                 |\n",
        "| **Similarity Basis** | Co-occurrence of items within sessions                                  |\n",
        "| **Algorithm**        | Compute item-item cosine similarity; aggregate scores for session items |\n",
        "| **Evaluation**       | Recall@K and MRR@K (same as popularity)                                 |\n",
        "| **Output**           | Personalized Top-K recommendations                                      |\n"
      ],
      "metadata": {
        "id": "RVQBp7wqjq-U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3.5 ‚Äî Item-Based kNN Recommender\n",
        "\n",
        "### Objective\n",
        "To build a **personalized, session-aware recommender** that suggests products based on their co-occurrence with other products in the same session.\n",
        "\n",
        "Unlike the **Popularity-Based Recommender**, which recommends the same top items to every user, this model captures **relationships between products**.  \n",
        "It uses a *k-nearest neighbors (k-NN)* approach, where ‚Äúneighbors‚Äù are **items that are often viewed together** in the same session.\n",
        "\n",
        "---\n",
        "\n",
        "### Why Item-Based kNN?\n",
        "In our dataset, users are mostly anonymous (identified by sessions, not persistent user IDs).  \n",
        "Hence, **User-Based Collaborative Filtering** isn‚Äôt suitable.  \n",
        "Instead, we apply **Item-Based Collaborative Filtering (CF)** which leverages co-occurrence patterns within sessions.\n",
        "\n",
        "---\n",
        "\n",
        "### Core Idea\n",
        "- If many sessions contain both *Item A* and *Item B*,  \n",
        "  ‚Üí then A and B are considered *similar*.  \n",
        "- For a new session `[A, D]`, the recommender looks up items similar to A and D,  \n",
        "  aggregates their similarity scores, and recommends the top items not yet viewed.\n",
        "\n",
        "---\n",
        "\n",
        "### Methodology Overview\n",
        "1. **Build an Item-Item Co-occurrence Matrix**  \n",
        "   - Count how many times each pair of items appears together in sessions.  \n",
        "\n",
        "2. **Compute Item Similarity Scores**  \n",
        "   - Use a normalized cosine-style measure to get similarity between items.  \n",
        "\n",
        "3. **Get k Nearest Neighbors**  \n",
        "   - For each item, store the top-k most similar items.  \n",
        "\n",
        "4. **Generate Recommendations for Test Sessions**  \n",
        "   - For a given session, aggregate similar items of the products already viewed,  \n",
        "     and rank them by cumulative similarity.  \n",
        "\n",
        "5. **Evaluate using Recall@K and MRR@K**  \n",
        "   - Same metrics as in the Popularity Model, for direct comparison.\n",
        "\n",
        "---\n",
        "\n",
        "### Business Intuition\n",
        "This model mimics the logic behind real-world product discovery:\n",
        "> ‚ÄúPeople who viewed this item also viewed...‚Äù  \n",
        "and provides **personalized suggestions** based on the user‚Äôs ongoing session ‚Äî  \n",
        "a crucial step towards intelligent, real-time e-commerce recommendations.\n",
        "\n",
        "---\n",
        "\n",
        "### Expected Outcome\n",
        "We expect **Recall@K** and **MRR@K** to be higher than the Popularity Baseline,  \n",
        "because the recommendations are now *contextualized to the user‚Äôs session*.\n",
        "\n",
        "---\n",
        "\n",
        "**Next:**  \n",
        "We‚Äôll implement the item co-occurrence logic and generate similarity-based recommendations.\n"
      ],
      "metadata": {
        "id": "IOOOnObAkR6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 3.5.1 ‚Äî Build Item Co-occurrence Matrix\n",
        "\n",
        "from collections import defaultdict\n",
        "import itertools\n",
        "\n",
        "# Use only training sessions for building similarity\n",
        "train_sids = set(split['train'])\n",
        "\n",
        "cooccur = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "for sid in train_sids:\n",
        "    items = sessions.get(sid)\n",
        "    if not items or len(items) < 2:\n",
        "        continue\n",
        "    # For each unique pair of items in a session, increment co-occurrence count\n",
        "    for i, j in itertools.combinations(set(items), 2):\n",
        "        cooccur[i][j] += 1\n",
        "        cooccur[j][i] += 1  # symmetric matrix\n",
        "\n",
        "# Count total number of co-occurrence relationships stored\n",
        "num_pairs = sum(len(v) for v in cooccur.values())\n",
        "print(f\"Built co-occurrence matrix with {len(cooccur)} items and {num_pairs} item-pairs.\")\n",
        "\n",
        "# a few items' neighbors\n",
        "sample_items = list(cooccur.keys())[:5]\n",
        "for item in sample_items:\n",
        "    print(f\"\\nItem {item} co-occurs with:\")\n",
        "    neighbors = list(cooccur[item].items())[:5]\n",
        "    for nb, cnt in neighbors:\n",
        "        print(f\"  ‚Üí Item {nb} ({cnt} times)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89ZRp80zjvVA",
        "outputId": "609278cd-ee3c-4826-ff10-931d2dcc6ea3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Built co-occurrence matrix with 25970 items and 316726 item-pairs.\n",
            "\n",
            "Item 3309 co-occurs with:\n",
            "  ‚Üí Item 8757 (3 times)\n",
            "  ‚Üí Item 3306 (2 times)\n",
            "  ‚Üí Item 3307 (1 times)\n",
            "  ‚Üí Item 3308 (2 times)\n",
            "  ‚Üí Item 3310 (1 times)\n",
            "\n",
            "Item 8757 co-occurs with:\n",
            "  ‚Üí Item 3309 (3 times)\n",
            "  ‚Üí Item 5249 (1 times)\n",
            "  ‚Üí Item 8751 (1 times)\n",
            "  ‚Üí Item 8752 (1 times)\n",
            "  ‚Üí Item 8753 (1 times)\n",
            "\n",
            "Item 27721 co-occurs with:\n",
            "  ‚Üí Item 27722 (1 times)\n",
            "  ‚Üí Item 27723 (1 times)\n",
            "  ‚Üí Item 27724 (1 times)\n",
            "  ‚Üí Item 27725 (1 times)\n",
            "  ‚Üí Item 26542 (1 times)\n",
            "\n",
            "Item 27722 co-occurs with:\n",
            "  ‚Üí Item 27721 (1 times)\n",
            "  ‚Üí Item 27723 (1 times)\n",
            "  ‚Üí Item 27724 (1 times)\n",
            "  ‚Üí Item 27725 (1 times)\n",
            "  ‚Üí Item 26542 (1 times)\n",
            "\n",
            "Item 27723 co-occurs with:\n",
            "  ‚Üí Item 27721 (1 times)\n",
            "  ‚Üí Item 27722 (1 times)\n",
            "  ‚Üí Item 27724 (1 times)\n",
            "  ‚Üí Item 27725 (1 times)\n",
            "  ‚Üí Item 26542 (1 times)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 3.5.2 ‚Äî Compute normalized item similarity scores\n",
        "from math import sqrt\n",
        "import heapq\n",
        "\n",
        "# 1. Compute frequency of each item (how many sessions contained it)\n",
        "freq = defaultdict(int)\n",
        "for sid in split['train']:\n",
        "    items = set(sessions.get(sid, []))\n",
        "    for i in items:\n",
        "        freq[i] += 1\n",
        "\n",
        "# 2. Convert co-occurrence counts to cosine-style similarity\n",
        "sim_matrix = defaultdict(dict)\n",
        "for i, neighbors in cooccur.items():\n",
        "    for j, cij in neighbors.items():\n",
        "        if freq[i] > 0 and freq[j] > 0:\n",
        "            sim = cij / sqrt(freq[i] * freq[j])\n",
        "            sim_matrix[i][j] = sim\n",
        "\n",
        "print(f\"Built similarity matrix for {len(sim_matrix)} items.\")\n",
        "\n",
        "# 3. Keep only top-N similar items per item (memory optimization)\n",
        "TOPN = 50\n",
        "for i, neighbors in sim_matrix.items():\n",
        "    top_neighbors = heapq.nlargest(TOPN, neighbors.items(), key=lambda x: x[1])\n",
        "    sim_matrix[i] = dict(top_neighbors)\n",
        "\n",
        "# Peek at a few examples\n",
        "for item in list(sim_matrix.keys())[:3]:\n",
        "    print(f\"\\nItem {item} top similar items:\")\n",
        "    for nb, score in list(sim_matrix[item].items())[:5]:\n",
        "        print(f\"  ‚Üí Item {nb} (sim={score:.3f})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b1pIX_Olqhb",
        "outputId": "aa17b11a-ab1c-4dd0-df8c-86d2403f88a8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Built similarity matrix for 25970 items.\n",
            "\n",
            "Item 3309 top similar items:\n",
            "  ‚Üí Item 8757 (sim=0.567)\n",
            "  ‚Üí Item 3310 (sim=0.500)\n",
            "  ‚Üí Item 3311 (sim=0.500)\n",
            "  ‚Üí Item 11042 (sim=0.500)\n",
            "  ‚Üí Item 11043 (sim=0.500)\n",
            "\n",
            "Item 8757 top similar items:\n",
            "  ‚Üí Item 3309 (sim=0.567)\n",
            "  ‚Üí Item 8756 (sim=0.567)\n",
            "  ‚Üí Item 3306 (sim=0.463)\n",
            "  ‚Üí Item 8755 (sim=0.378)\n",
            "  ‚Üí Item 8758 (sim=0.378)\n",
            "\n",
            "Item 27721 top similar items:\n",
            "  ‚Üí Item 27722 (sim=1.000)\n",
            "  ‚Üí Item 27723 (sim=1.000)\n",
            "  ‚Üí Item 27724 (sim=1.000)\n",
            "  ‚Üí Item 27725 (sim=1.000)\n",
            "  ‚Üí Item 27726 (sim=1.000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 3.5.3 Generate item based kNN recommendations for each test session and evaluate\n",
        "\n",
        "import os, json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "# Settings\n",
        "K = 20          # evaluation cutoff\n",
        "AGG_TOPN = 100  # how many candidate neighbors to consider (per session aggregation)\n",
        "RESULTS_DIR = os.path.join(PROJECT_ROOT, 'results')\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "# Prepare test examples - session_id, history, target\n",
        "test_sids = set(split['test'])\n",
        "test_examples = []\n",
        "for sid in test_sids:\n",
        "    seq = sessions.get(sid)\n",
        "    if not seq or len(seq) < 2:\n",
        "        continue\n",
        "    history = seq[:-1]  # input history\n",
        "    target = seq[-1]  # ground truth next item\n",
        "    test_examples.append((sid, history, target))\n",
        "\n",
        "print(f\"Prepared {len(test_examples)} test examples for kNN evaluation ...\")\n",
        "\n",
        "def recall_at_k(preds, target, k):\n",
        "    return 1.0 if target in preds[:k] else 0.0\n",
        "\n",
        "def mrr_at_k(preds, target, k):\n",
        "    for i, p in enumerate(preds[:k], start=1):\n",
        "        if p == target:\n",
        "            return 1.0 / i\n",
        "    return 0.0\n",
        "\n",
        "# Recommendation function using sim_matrix\n",
        "def recommend_for_history(history, sim_matrix, top_k=K, agg_topn=AGG_TOPN):\n",
        "    score = defaultdict(float)\n",
        "    seen = set(history)\n",
        "    # Aggregate neighbors for each item in history\n",
        "    for item in history:\n",
        "        neighbors = sim_matrix.get(item, {})\n",
        "        # accumulate neighbor similarity scores\n",
        "        for nb, sim in neighbors.items():\n",
        "            if nb in seen:\n",
        "                continue  # skip already-seen items (we don't wanna recommend already seen items)\n",
        "            score[nb] += sim\n",
        "    # If no candidates (cold case), fallback to popularity (popular_items must exist)\n",
        "    if not score:\n",
        "        # popular_items is list from earlier popularity baseline\n",
        "        return popular_items[:top_k]\n",
        "    # Sort candidates by aggregated score\n",
        "    # limit to agg_topn candidates for speed\n",
        "    candidates = sorted(score.items(), key=lambda x: x[1], reverse=True)[:agg_topn]\n",
        "    preds = [item for item, _ in candidates][:top_k]\n",
        "    return preds\n",
        "\n",
        "# Evaluate on test set\n",
        "recalls = []\n",
        "mrrs = []\n",
        "pred_rows = []\n",
        "for sid, history, target in tqdm(test_examples, desc=\"Evaluating k-NN\"):\n",
        "    preds = recommend_for_history(history, sim_matrix, top_k=K)\n",
        "    recalls.append(recall_at_k(preds, target, K))\n",
        "    mrrs.append(mrr_at_k(preds, target, K))\n",
        "    # save example row (first 20)\n",
        "    if len(pred_rows) < 100:\n",
        "        pred_rows.append({\n",
        "            \"session_id\": sid,\n",
        "            \"history\": history,\n",
        "            \"ground_truth\": target,\n",
        "            \"top20_predictions\": preds\n",
        "        })\n",
        "\n",
        "# Compute metrics\n",
        "recall_k = float(np.mean(recalls)) if recalls else 0.0\n",
        "mrr_k = float(np.mean(mrrs)) if mrrs else 0.0\n",
        "\n",
        "knn_metrics = {\n",
        "    \"model\": \"item_knn\",\n",
        "    \"K\": K,\n",
        "    \"recall@K\": round(recall_k, 4),\n",
        "    \"mrr@K\": round(mrr_k, 4),\n",
        "    \"num_test_sessions\": len(recalls)\n",
        "}\n",
        "\n",
        "# Save metrics and sample predictions\n",
        "metrics_fp = os.path.join(RESULTS_DIR, 'knn_metrics.json')\n",
        "with open(metrics_fp, 'w') as f:\n",
        "    json.dump(knn_metrics, f, indent=2)\n",
        "\n",
        "preds_fp = os.path.join(RESULTS_DIR, 'knn_predictions_sample.csv')\n",
        "pd.DataFrame(pred_rows).to_csv(preds_fp, index=False)\n",
        "\n",
        "print(\"\\Item-based k-NN Results\")\n",
        "for k, v in knn_metrics.items():\n",
        "    print(f\"{k:20}: {v}\")\n",
        "\n",
        "print(f\"\\Metrics saved to: {metrics_fp}\")\n",
        "print(f\"Sample predictions saved to: {preds_fp}\")"
      ],
      "metadata": {
        "id": "Y1kUIJRBqvcL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1913768-d801-45dd-ef57-f8bd440d873e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:96: SyntaxWarning: invalid escape sequence '\\I'\n",
            "<>:100: SyntaxWarning: invalid escape sequence '\\M'\n",
            "<>:96: SyntaxWarning: invalid escape sequence '\\I'\n",
            "<>:100: SyntaxWarning: invalid escape sequence '\\M'\n",
            "/tmp/ipython-input-2658169914.py:96: SyntaxWarning: invalid escape sequence '\\I'\n",
            "  print(\"\\Item-based k-NN Results\")\n",
            "/tmp/ipython-input-2658169914.py:100: SyntaxWarning: invalid escape sequence '\\M'\n",
            "  print(f\"\\Metrics saved to: {metrics_fp}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepared 1640 test examples for kNN evaluation ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating k-NN: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1640/1640 [00:00<00:00, 29372.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\Item-based k-NN Results\n",
            "model               : item_knn\n",
            "K                   : 20\n",
            "recall@K            : 0.1799\n",
            "mrr@K               : 0.0484\n",
            "num_test_sessions   : 1640\n",
            "\\Metrics saved to: /content/drive/MyDrive/Data Science course/Major Projects/Projects/e-commerce recommender/diginetica_recommender_project/results/knn_metrics.json\n",
            "Sample predictions saved to: /content/drive/MyDrive/Data Science course/Major Projects/Projects/e-commerce recommender/diginetica_recommender_project/results/knn_predictions_sample.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adf1eee1",
        "outputId": "bf8df01f-943d-4215-fb4a-30b172b2c8d4"
      },
      "source": [
        "# Display the kNN metrics\n",
        "print(\"\\nItem-based k-NN Results\")\n",
        "for k, v in knn_metrics.items():\n",
        "    print(f\"{k:20}: {v}\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Item-based k-NN Results\n",
            "model               : item_knn\n",
            "K                   : 20\n",
            "recall@K            : 0.1799\n",
            "mrr@K               : 0.0484\n",
            "num_test_sessions   : 1640\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary ‚Äî Baseline Models (Popularity vs. Item-kNN)\n",
        "\n",
        "| Model | Recall@20 | MRR@20 |\n",
        "|--------|------------|--------|\n",
        "| Popularity | 0.0116 | 0.0028 |\n",
        "| Item-kNN | **0.1799** | **0.0484** |\n",
        "\n",
        "### Observations\n",
        "- The **Item-kNN Recommender** shows a massive improvement in both Recall and MRR.\n",
        "- This confirms that **session context** (recently viewed items) is critical for personalization.\n",
        "- Even without explicit machine learning, the co-occurrence + similarity logic captures strong behavioral patterns.\n",
        "\n",
        "### Key Learnings\n",
        "1. Popularity models serve as a fast, robust fallback.\n",
        "2. Item-based kNN introduces data-driven personalization with minimal complexity.\n",
        "3. These baselines form a solid foundation for deep learning models like **GRU4Rec** and **SASRec**.\n",
        "\n",
        "### Outputs\n",
        "| File | Description |\n",
        "|------|--------------|\n",
        "| `baseline_metrics.json` | Popularity model metrics |\n",
        "| `knn_metrics.json` | Item-kNN model metrics |\n",
        "| `baseline_predictions.csv` | Popularity sample predictions |\n",
        "| `knn_predictions_sample.csv` | Item-kNN sample predictions |\n",
        "\n",
        "**Next:** Move to **Notebook 4 ‚Äî Advanced Sequential Models**  \n",
        "to train and evaluate a **GRU4Rec** model that learns dynamic, temporal user behavior directly from session sequences.\n"
      ],
      "metadata": {
        "id": "PhOcMUbudtDy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7qRJ4PQzVYJI"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}